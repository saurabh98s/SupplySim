{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/19 16:46:50 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.013185391907327442\n",
      "R-squared: 0.9926454894831901\n",
      "\n",
      "Top 10 Important Features:\n",
      "                    Feature  Importance\n",
      "2   Number of products sold    0.767571\n",
      "0                     Price    0.219490\n",
      "7            Shipping costs    0.001033\n",
      "1              Availability    0.000952\n",
      "12                    Costs    0.000902\n",
      "5          Order quantities    0.000884\n",
      "13            Demand Factor    0.000839\n",
      "4                Lead times    0.000729\n",
      "11             Defect rates    0.000702\n",
      "8        Production volumes    0.000696\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "# Load the cleaned dataset\n",
    "data = pd.read_csv('S:/SJSU/DATA_226/group_project/data/processed/cleaned_improved_dataset.csv')\n",
    "\n",
    "# 1. Feature Engineering\n",
    "# Include \"SKU\" in the categorical columns for one-hot encoding\n",
    "categorical_columns = [\"Product type\", \"SKU\", \"Customer demographics\", \"Shipping carriers\", \n",
    "                       \"Supplier name\", \"Location\", \"Inspection results\", \n",
    "                       \"Transportation modes\", \"Routes\", \"Season\"]\n",
    "\n",
    "# One-hot encoding for all categorical variables, including SKU\n",
    "data_encoded = pd.get_dummies(data, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Extract features and target variable\n",
    "X = data_encoded.drop(columns=[\"Revenue generated\"])  # Features\n",
    "y = data_encoded[\"Revenue generated\"]  # Target\n",
    "\n",
    "# Identify the numeric columns for transformations\n",
    "numeric_columns = X.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "X[numeric_columns] = scaler.fit_transform(X[numeric_columns])\n",
    "\n",
    "# Cyclical Encoding for Temporal Features\n",
    "season_mapping = {\"Winter\": 0, \"Spring\": 1, \"Summer\": 2, \"Fall\": 3}\n",
    "data_encoded[\"Season_Cyclical\"] = data[\"Season\"].map(season_mapping)\n",
    "X[\"Season_Sin\"] = np.sin(2 * np.pi * data_encoded[\"Season_Cyclical\"] / 4)\n",
    "X[\"Season_Cos\"] = np.cos(2 * np.pi * data_encoded[\"Season_Cyclical\"] / 4)\n",
    "X = X.drop(columns=[\"Season_Cyclical\"], errors='ignore')\n",
    "\n",
    "# 2. Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Start an MLflow run\n",
    "mlflow.start_run()\n",
    "\n",
    "# 3. Model Training: Random Forest Regressor\n",
    "model = RandomForestRegressor(n_estimators=400, random_state=45)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 4. Model Evaluation\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Log parameters and metrics to MLflow\n",
    "mlflow.log_param(\"n_estimators\", 400)\n",
    "mlflow.log_param(\"random_state\", 45)\n",
    "mlflow.log_metric(\"mse\", mse)\n",
    "mlflow.log_metric(\"r2\", r2)\n",
    "\n",
    "# Log the model\n",
    "mlflow.sklearn.log_model(model, \"random_forest_model\")\n",
    "\n",
    "# End the MLflow run\n",
    "mlflow.end_run()\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r2}\")\n",
    "\n",
    "# 5. Feature Importance Analysis\n",
    "importances = model.feature_importances_\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    \"Feature\": X.columns,\n",
    "    \"Importance\": importances\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# Print the top 10 important features\n",
    "print(\"\\nTop 10 Important Features:\")\n",
    "print(feature_importance_df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/19 16:49:24 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'n_estimators': 100,\n",
       "  'min_samples_split': 2,\n",
       "  'min_samples_leaf': 2,\n",
       "  'max_features': 0.75,\n",
       "  'max_depth': None},\n",
       " np.float64(0.02533564904186372),\n",
       " 0.9858683535052877)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Correcting the hyperparameter grid to remove the invalid 'auto' option\n",
    "param_distributions = {\n",
    "    \"n_estimators\": [100, 200, 300, 400, 500],\n",
    "    \"max_depth\": [10, 15, 20, 25, 30, None],\n",
    "    \"min_samples_split\": [2, 5, 10, 15],\n",
    "    \"min_samples_leaf\": [1, 2, 5, 10],\n",
    "    \"max_features\": [\"sqrt\", \"log2\", 0.5, 0.75]  # Removed 'auto' and kept valid options\n",
    "}\n",
    "\n",
    "# Initialize the Random Forest model\n",
    "model = RandomForestRegressor(random_state=45)\n",
    "\n",
    "# Set up RandomizedSearchCV with the corrected parameter grid\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=model,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=20,  # Number of parameter settings to sample\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    verbose=2,  # Print progress\n",
    "    random_state=42,\n",
    "    n_jobs=-1  # Use all available cores\n",
    ")\n",
    "\n",
    "# Fit the RandomizedSearchCV to the training data\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Extract the best model and hyperparameters\n",
    "best_model = random_search.best_estimator_\n",
    "best_params = random_search.best_params_\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Log the best parameters and metrics to MLflow\n",
    "mlflow.start_run()\n",
    "mlflow.log_params(best_params)\n",
    "mlflow.log_metric(\"mse\", mse)\n",
    "mlflow.log_metric(\"r2\", r2)\n",
    "mlflow.sklearn.log_model(best_model, \"random_forest_best_model\")\n",
    "mlflow.end_run()\n",
    "\n",
    "# Print the best hyperparameters and metrics\n",
    "best_params, mse, r2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/19 17:03:26 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.013185391907327442\n",
      "R-squared: 0.9926454894831901\n",
      "\n",
      "Top 10 Important Features:\n",
      "                    Feature  Importance\n",
      "2   Number of products sold    0.767571\n",
      "0                     Price    0.219490\n",
      "7            Shipping costs    0.001033\n",
      "1              Availability    0.000952\n",
      "12                    Costs    0.000902\n",
      "5          Order quantities    0.000884\n",
      "13            Demand Factor    0.000839\n",
      "4                Lead times    0.000729\n",
      "11             Defect rates    0.000702\n",
      "8        Production volumes    0.000696\n",
      "Predicted Inventory Metrics: [7.24355085]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "# Load the cleaned dataset\n",
    "data = pd.read_csv('S:/SJSU/DATA_226/group_project/data/processed/cleaned_improved_dataset.csv')\n",
    "\n",
    "# 1. Feature Engineering\n",
    "# Include \"SKU\" in the categorical columns for one-hot encoding\n",
    "categorical_columns = [\"Product type\", \"SKU\", \"Customer demographics\", \"Shipping carriers\", \n",
    "                       \"Supplier name\", \"Location\", \"Inspection results\", \n",
    "                       \"Transportation modes\", \"Routes\", \"Season\"]\n",
    "\n",
    "# One-hot encoding for all categorical variables, including SKU\n",
    "data_encoded = pd.get_dummies(data, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Extract features and target variable\n",
    "X = data_encoded.drop(columns=[\"Revenue generated\"])  # Features\n",
    "y = data_encoded[\"Revenue generated\"]  # Target\n",
    "\n",
    "# Identify the numeric columns for transformations\n",
    "numeric_columns = X.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "X[numeric_columns] = scaler.fit_transform(X[numeric_columns])\n",
    "\n",
    "# Cyclical Encoding for Temporal Features\n",
    "season_mapping = {\"Winter\": 0, \"Spring\": 1, \"Summer\": 2, \"Fall\": 3}\n",
    "data_encoded[\"Season_Cyclical\"] = data[\"Season\"].map(season_mapping)\n",
    "X[\"Season_Sin\"] = np.sin(2 * np.pi * data_encoded[\"Season_Cyclical\"] / 4)\n",
    "X[\"Season_Cos\"] = np.cos(2 * np.pi * data_encoded[\"Season_Cyclical\"] / 4)\n",
    "X = X.drop(columns=[\"Season_Cyclical\"], errors='ignore')\n",
    "\n",
    "# 2. Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Start an MLflow run\n",
    "mlflow.start_run()\n",
    "\n",
    "# 3. Model Training: Random Forest Regressor\n",
    "model = RandomForestRegressor(n_estimators=400, random_state=45)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 4. Model Evaluation\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Log parameters and metrics to MLflow\n",
    "mlflow.log_param(\"n_estimators\", 400)\n",
    "mlflow.log_param(\"random_state\", 45)\n",
    "mlflow.log_metric(\"mse\", mse)\n",
    "mlflow.log_metric(\"r2\", r2)\n",
    "\n",
    "# Log feature importances to MLflow\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    \"Feature\": X.columns,\n",
    "    \"Importance\": model.feature_importances_\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "mlflow.log_text(feature_importance_df.to_string(), \"feature_importance.txt\")\n",
    "\n",
    "# Log the model\n",
    "mlflow.sklearn.log_model(model, \"random_forest_model\")\n",
    "\n",
    "# End the MLflow run\n",
    "mlflow.end_run()\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r2}\")\n",
    "\n",
    "# Print the top 10 important features\n",
    "print(\"\\nTop 10 Important Features:\")\n",
    "print(feature_importance_df.head(10))\n",
    "\n",
    "# 5. Warehouse Management Predictions\n",
    "def predict_inventory_management(new_data):\n",
    "    \"\"\"\n",
    "    Predict warehouse metrics using the trained model.\n",
    "    \"\"\"\n",
    "    # One-hot encode new data and align it with training data\n",
    "    new_data_encoded = pd.get_dummies(new_data, columns=categorical_columns, drop_first=True)\n",
    "    \n",
    "    # Identify missing columns and convert the set to a list\n",
    "    missing_cols = list(set(X.columns) - set(new_data_encoded.columns))\n",
    "    \n",
    "    # Add missing columns with default value 0\n",
    "    new_data_encoded = pd.concat([new_data_encoded, pd.DataFrame(0, index=new_data_encoded.index, columns=missing_cols)], axis=1)\n",
    "    \n",
    "    # Reorder columns to match the training set\n",
    "    new_data_encoded = new_data_encoded[X.columns]\n",
    "    \n",
    "    # Standardize numeric columns\n",
    "    new_data_encoded[numeric_columns] = scaler.transform(new_data_encoded[numeric_columns])\n",
    "    \n",
    "    # Make predictions\n",
    "    predicted_values = model.predict(new_data_encoded)\n",
    "    return predicted_values\n",
    "\n",
    "\n",
    "# Example usage for warehouse management\n",
    "new_data = pd.DataFrame({\n",
    "    \"Product type\": [\"haircare\"],\n",
    "    \"SKU\": [\"SKU45\"],\n",
    "    \"Price\": [25.5],\n",
    "    \"Availability\": [100],\n",
    "    \"Number of products sold\": [50],\n",
    "    \"Customer demographics\": [\"Female\"],\n",
    "    \"Stock levels\": [80],\n",
    "    \"Lead times\": [5],\n",
    "    \"Order quantities\": [30],\n",
    "    \"Shipping times\": [3],\n",
    "    \"Shipping carriers\": [\"Carrier A\"],\n",
    "    \"Shipping costs\": [12.5],\n",
    "    \"Supplier name\": [\"Supplier 1\"],\n",
    "    \"Location\": [\"Mumbai\"],\n",
    "    \"Production volumes\": [500],\n",
    "    \"Manufacturing lead time\": [10],\n",
    "    \"Manufacturing costs\": [35.0],\n",
    "    \"Inspection results\": [\"Pass\"],\n",
    "    \"Defect rates\": [0.5],\n",
    "    \"Transportation modes\": [\"Road\"],\n",
    "    \"Routes\": [\"Route A\"],\n",
    "    \"Costs\": [150.0],\n",
    "    \"Season\": [\"Winter\"],\n",
    "    \"Demand Factor\": [1.3]\n",
    "})\n",
    "\n",
    "# Generate and print predictions\n",
    "predicted_values = predict_inventory_management(new_data)\n",
    "print(\"Predicted Inventory Metrics:\", predicted_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MultiRegressor Output Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/19 17:26:57 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Errors: [  0.           0.           0.         112.59510873]\n",
      "R-squared: 0.9918138387587876\n",
      "Predicted Inventory Metrics: {'SKU': 'SKU45', 'Restock Indicator': 1, 'Restock Date (days)': 7, 'Restock Quantity': 10, 'Predicted Costs': np.float64(203.94)}\n",
      "Predicted Inventory Metrics: {'SKU': 'SKU46', 'Restock Indicator': 1, 'Restock Date (days)': 7, 'Restock Quantity': 10, 'Predicted Costs': np.float64(254.19)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saura\\AppData\\Local\\Temp\\ipykernel_20016\\2830221470.py:95: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data_encoded[col] = 0  # Add missing columns with default value 0\n",
      "C:\\Users\\saura\\AppData\\Local\\Temp\\ipykernel_20016\\2830221470.py:95: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data_encoded[col] = 0  # Add missing columns with default value 0\n",
      "C:\\Users\\saura\\AppData\\Local\\Temp\\ipykernel_20016\\2830221470.py:95: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data_encoded[col] = 0  # Add missing columns with default value 0\n",
      "C:\\Users\\saura\\AppData\\Local\\Temp\\ipykernel_20016\\2830221470.py:95: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data_encoded[col] = 0  # Add missing columns with default value 0\n",
      "C:\\Users\\saura\\AppData\\Local\\Temp\\ipykernel_20016\\2830221470.py:95: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data_encoded[col] = 0  # Add missing columns with default value 0\n",
      "C:\\Users\\saura\\AppData\\Local\\Temp\\ipykernel_20016\\2830221470.py:95: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data_encoded[col] = 0  # Add missing columns with default value 0\n",
      "C:\\Users\\saura\\AppData\\Local\\Temp\\ipykernel_20016\\2830221470.py:95: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data_encoded[col] = 0  # Add missing columns with default value 0\n",
      "C:\\Users\\saura\\AppData\\Local\\Temp\\ipykernel_20016\\2830221470.py:95: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data_encoded[col] = 0  # Add missing columns with default value 0\n",
      "C:\\Users\\saura\\AppData\\Local\\Temp\\ipykernel_20016\\2830221470.py:95: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data_encoded[col] = 0  # Add missing columns with default value 0\n",
      "C:\\Users\\saura\\AppData\\Local\\Temp\\ipykernel_20016\\2830221470.py:95: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data_encoded[col] = 0  # Add missing columns with default value 0\n",
      "C:\\Users\\saura\\AppData\\Local\\Temp\\ipykernel_20016\\2830221470.py:95: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data_encoded[col] = 0  # Add missing columns with default value 0\n",
      "C:\\Users\\saura\\AppData\\Local\\Temp\\ipykernel_20016\\2830221470.py:95: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data_encoded[col] = 0  # Add missing columns with default value 0\n",
      "C:\\Users\\saura\\AppData\\Local\\Temp\\ipykernel_20016\\2830221470.py:95: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data_encoded[col] = 0  # Add missing columns with default value 0\n",
      "C:\\Users\\saura\\AppData\\Local\\Temp\\ipykernel_20016\\2830221470.py:95: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data_encoded[col] = 0  # Add missing columns with default value 0\n",
      "C:\\Users\\saura\\AppData\\Local\\Temp\\ipykernel_20016\\2830221470.py:95: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data_encoded[col] = 0  # Add missing columns with default value 0\n",
      "C:\\Users\\saura\\AppData\\Local\\Temp\\ipykernel_20016\\2830221470.py:95: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data_encoded[col] = 0  # Add missing columns with default value 0\n",
      "C:\\Users\\saura\\AppData\\Local\\Temp\\ipykernel_20016\\2830221470.py:95: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data_encoded[col] = 0  # Add missing columns with default value 0\n",
      "C:\\Users\\saura\\AppData\\Local\\Temp\\ipykernel_20016\\2830221470.py:95: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data_encoded[col] = 0  # Add missing columns with default value 0\n",
      "C:\\Users\\saura\\AppData\\Local\\Temp\\ipykernel_20016\\2830221470.py:95: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data_encoded[col] = 0  # Add missing columns with default value 0\n",
      "C:\\Users\\saura\\AppData\\Local\\Temp\\ipykernel_20016\\2830221470.py:95: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data_encoded[col] = 0  # Add missing columns with default value 0\n",
      "C:\\Users\\saura\\AppData\\Local\\Temp\\ipykernel_20016\\2830221470.py:95: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data_encoded[col] = 0  # Add missing columns with default value 0\n",
      "C:\\Users\\saura\\AppData\\Local\\Temp\\ipykernel_20016\\2830221470.py:95: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data_encoded[col] = 0  # Add missing columns with default value 0\n",
      "C:\\Users\\saura\\AppData\\Local\\Temp\\ipykernel_20016\\2830221470.py:95: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data_encoded[col] = 0  # Add missing columns with default value 0\n",
      "C:\\Users\\saura\\AppData\\Local\\Temp\\ipykernel_20016\\2830221470.py:95: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data_encoded[col] = 0  # Add missing columns with default value 0\n",
      "C:\\Users\\saura\\AppData\\Local\\Temp\\ipykernel_20016\\2830221470.py:95: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data_encoded[col] = 0  # Add missing columns with default value 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "# Load the cleaned dataset\n",
    "data = pd.read_csv('S:/SJSU/DATA_226/group_project/data/processed/cleaned_improved_dataset.csv')\n",
    "\n",
    "# 1. Feature Engineering\n",
    "# Include \"SKU\" in the categorical columns for one-hot encoding\n",
    "categorical_columns = [\"Product type\", \"SKU\", \"Customer demographics\", \"Shipping carriers\", \n",
    "                       \"Supplier name\", \"Location\", \"Inspection results\", \n",
    "                       \"Transportation modes\", \"Routes\", \"Season\"]\n",
    "\n",
    "# One-hot encoding for all categorical variables, including SKU\n",
    "data_encoded = pd.get_dummies(data, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Extract features\n",
    "X = data_encoded.drop(columns=[\"Revenue generated\"])  # Features\n",
    "\n",
    "# Derive multiple target variables for the warehouse use case\n",
    "data_encoded[\"Restock Indicator\"] = (data_encoded[\"Stock levels\"] < data_encoded[\"Demand Factor\"] * 10).astype(int)\n",
    "data_encoded[\"Restock Date\"] = data_encoded[\"Lead times\"]  # Example: Use lead times as a proxy for restock date\n",
    "data_encoded[\"Restock Quantity\"] = np.maximum(data_encoded[\"Demand Factor\"] * 5, 10)\n",
    "data_encoded[\"Predicted Costs\"] = data_encoded[\"Costs\"] + data_encoded[\"Shipping costs\"] + data_encoded[\"Manufacturing costs\"]\n",
    "\n",
    "# Define target variables\n",
    "y = data_encoded[[\"Restock Indicator\", \"Restock Date\", \"Restock Quantity\", \"Predicted Costs\"]]\n",
    "\n",
    "# Identify the numeric columns for transformations\n",
    "numeric_columns = X.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "X[numeric_columns] = scaler.fit_transform(X[numeric_columns])\n",
    "\n",
    "# Cyclical Encoding for Temporal Features\n",
    "season_mapping = {\"Winter\": 0, \"Spring\": 1, \"Summer\": 2, \"Fall\": 3}\n",
    "data_encoded[\"Season_Cyclical\"] = data[\"Season\"].map(season_mapping)\n",
    "X[\"Season_Sin\"] = np.sin(2 * np.pi * data_encoded[\"Season_Cyclical\"] / 4)\n",
    "X[\"Season_Cos\"] = np.cos(2 * np.pi * data_encoded[\"Season_Cyclical\"] / 4)\n",
    "X = X.drop(columns=[\"Season_Cyclical\"], errors='ignore')\n",
    "\n",
    "# 2. Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Start an MLflow run\n",
    "mlflow.start_run()\n",
    "\n",
    "# 3. Multi-Output Model Training: Random Forest Regressor\n",
    "base_model = RandomForestRegressor(n_estimators=400, random_state=45)\n",
    "multi_output_model = MultiOutputRegressor(base_model)\n",
    "multi_output_model.fit(X_train, y_train)\n",
    "\n",
    "# 4. Model Evaluation\n",
    "y_pred = multi_output_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred, multioutput='raw_values')\n",
    "r2 = r2_score(y_test, y_pred, multioutput='variance_weighted')\n",
    "\n",
    "# Log parameters and metrics to MLflow\n",
    "mlflow.log_param(\"n_estimators\", 400)\n",
    "mlflow.log_param(\"random_state\", 45)\n",
    "mlflow.log_metric(\"mse_restock_indicator\", mse[0])\n",
    "mlflow.log_metric(\"mse_restock_date\", mse[1])\n",
    "mlflow.log_metric(\"mse_restock_quantity\", mse[2])\n",
    "mlflow.log_metric(\"mse_predicted_costs\", mse[3])\n",
    "mlflow.log_metric(\"r2_score\", r2)\n",
    "\n",
    "# Log the model\n",
    "mlflow.sklearn.log_model(multi_output_model, \"multi_output_random_forest_model\")\n",
    "\n",
    "# End the MLflow run\n",
    "mlflow.end_run()\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Mean Squared Errors: {mse}\")\n",
    "print(f\"R-squared: {r2}\")\n",
    "\n",
    "# Extended Prediction Function\n",
    "def predict_inventory_management(new_data):\n",
    "    \"\"\"\n",
    "    Predict warehouse metrics for multiple SKUs using the trained multi-output model.\n",
    "    \"\"\"\n",
    "    # One-hot encode new data and align it with training data\n",
    "    new_data_encoded = pd.get_dummies(new_data, columns=categorical_columns, drop_first=True)\n",
    "    \n",
    "    # Identify and add any missing columns to match the training data\n",
    "    missing_cols = list(set(X.columns) - set(new_data_encoded.columns))\n",
    "    for col in missing_cols:\n",
    "        new_data_encoded[col] = 0  # Add missing columns with default value 0\n",
    "\n",
    "    # Reorder columns to match the training set\n",
    "    new_data_encoded = new_data_encoded[X.columns]\n",
    "\n",
    "    # Standardize numeric columns\n",
    "    new_data_encoded[numeric_columns] = scaler.transform(new_data_encoded[numeric_columns])\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = multi_output_model.predict(new_data_encoded)\n",
    "\n",
    "    # Format the results for each SKU\n",
    "    inventory_metrics = []\n",
    "    for i in range(len(new_data)):\n",
    "        metrics = {\n",
    "            \"SKU\": new_data[\"SKU\"].iloc[i],\n",
    "            \"Restock Indicator\": int(predictions[i][0]),\n",
    "            \"Restock Date (days)\": int(predictions[i][1]),\n",
    "            \"Restock Quantity\": int(predictions[i][2]),\n",
    "            \"Predicted Costs\": round(predictions[i][3], 2)\n",
    "        }\n",
    "        inventory_metrics.append(metrics)\n",
    "\n",
    "    return inventory_metrics\n",
    "\n",
    "# Example usage for multiple SKUs\n",
    "new_data = pd.DataFrame({\n",
    "    \"Product type\": [\"haircare\", \"skincare\"],\n",
    "    \"SKU\": [\"SKU45\", \"SKU46\"],\n",
    "    \"Price\": [25.5, 40.0],\n",
    "    \"Availability\": [100, 150],\n",
    "    \"Number of products sold\": [50, 60],\n",
    "    \"Customer demographics\": [\"Female\", \"Male\"],\n",
    "    \"Stock levels\": [80, 90],\n",
    "    \"Lead times\": [5, 7],\n",
    "    \"Order quantities\": [30, 40],\n",
    "    \"Shipping times\": [3, 4],\n",
    "    \"Shipping carriers\": [\"Carrier A\", \"Carrier B\"],\n",
    "    \"Shipping costs\": [12.5, 15.0],\n",
    "    \"Supplier name\": [\"Supplier 1\", \"Supplier 2\"],\n",
    "    \"Location\": [\"Mumbai\", \"Delhi\"],\n",
    "    \"Production volumes\": [500, 700],\n",
    "    \"Manufacturing lead time\": [10, 12],\n",
    "    \"Manufacturing costs\": [35.0, 50.0],\n",
    "    \"Inspection results\": [\"Pass\", \"Fail\"],\n",
    "    \"Defect rates\": [0.5, 1.0],\n",
    "    \"Transportation modes\": [\"Road\", \"Air\"],\n",
    "    \"Routes\": [\"Route A\", \"Route B\"],\n",
    "    \"Costs\": [150.0, 200.0],\n",
    "    \"Season\": [\"Winter\", \"Summer\"],\n",
    "    \"Demand Factor\": [1.3, 1.5]\n",
    "})\n",
    "\n",
    "# Generate and print predictions for multiple SKUs\n",
    "predicted_values = predict_inventory_management(new_data)\n",
    "for item in predicted_values:\n",
    "    print(\"Predicted Inventory Metrics:\", item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "group_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
